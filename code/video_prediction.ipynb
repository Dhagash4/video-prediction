{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5oKKua3xOnvW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from models.dcgan import *\n",
    "from models.lstm import *\n",
    "from data.MMNIST.mmnist import *\n",
    "from utils.visualizations import *\n",
    "from utils.TrainerFP import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_stats(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a, b = [5,[6,7,8]]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DCGANEncoder(in_size = (1,128, 128))\n",
    "# model = model.to(device)\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "model = models.vgg16()\n",
    "model = model.to(device)\n",
    "\n",
    "# summary(model, (3,64, 64))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.resnet import *\n",
    "\n",
    "model = Resnet18Encoder(in_size = (1,128,128))\n",
    "model = model.to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, (1,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmnist_data_dir = \"data/MMNIST/\" \n",
    "train_loader, val_loader,test_loader = MMNIST(mmnist_data_dir,batch_size = 20, seq_first = True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b = next(iter(val_loader))\n",
    "b1 = next(iter(val_loader))\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_grid_batch(b, b1, nsamples = 5, text = \"random\", batch_first = False, show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pred_gifs(b, nsamples=5, text = \"test\", batch_first = False, show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.KTH.kth import *\n",
    "from data.MMNIST.mmnist import *\n",
    "\n",
    "kth_data_dir = \"data/KTH/kth/\"\n",
    "mmnist_data_dir = \"data/MMNIST/\" \n",
    "\n",
    "batch_size = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader, val_loader,test_loader = get_KTH(kth_data_dir, batch_size = 20, seq_first=True, frame_skip=20, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"KTH Data Statistics:\")\n",
    "print(f\"Train loader len: {len(train_loader)}, Total training sequences: {len(train_loader) * batch_size}\")\n",
    "print(f\"Val loader len: {len(val_loader)}, Total validation sequences: {len(val_loader) * batch_size}\")\n",
    "print(f\"Test loader len: {len(test_loader)}, Total testing sequences: {len(test_loader) * batch_size}\")\n",
    "print(f\"Shape of batch: {sample_batch[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_loader:\n",
    "    print(x[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils.TrainerNP import *\n",
    "trainer = TrainerNP(arch_type = \"resnet\", img_size = (1,128,128),device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/ranaa0/video-prediction/code/data/KTH/kth.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[i]['seq'] = torch.tensor(batch[i]['seq']) / 255.0\n",
      "  0%|                                                                                                                                                                                                                                                                                                | 0/191 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 10.76 GiB total capacity; 9.53 GiB already allocated; 66.50 MiB free; 9.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/video-prediction/code/utils/TrainerNP.py:142\u001b[0m, in \u001b[0;36mTrainerNP.train\u001b[0;34m(self, train_loader, val_loader, test_loader, num_epochs, device, init_step)\u001b[0m\n\u001b[1;32m    140\u001b[0m     seqs \u001b[38;5;241m=\u001b[39m seqs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    141\u001b[0m seqs \u001b[38;5;241m=\u001b[39m seqs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 142\u001b[0m mse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m epoch_mse\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mmse\n\u001b[1;32m    144\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Iter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mniter\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: mse loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_mse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/video-prediction/code/utils/TrainerNP.py:60\u001b[0m, in \u001b[0;36mTrainerNP.train_one_step\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mh, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39minit_state()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# print(\"hello\")\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m h_seq \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpast_frames\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture_frames)]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# print(h_seq[0][0].shape)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m mse \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/video-prediction/code/utils/TrainerNP.py:60\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mh, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39minit_state()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# print(\"hello\")\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m h_seq \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpast_frames\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture_frames)]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# print(h_seq[0][0].shape)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m mse \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/visionproject/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/video-prediction/code/models/resnet.py:121\u001b[0m, in \u001b[0;36mResnet18Encoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    117\u001b[0m     \n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m#output list for skip connection\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     encoded_skip \u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     encoded_skip\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    124\u001b[0m     l1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(\u001b[38;5;28minput\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/visionproject/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/visionproject/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/visionproject/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/visionproject/lib/python3.9/site-packages/torch/nn/modules/conv.py:446\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/visionproject/lib/python3.9/site-packages/torch/nn/modules/conv.py:442\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    440\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    441\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 10.76 GiB total capacity; 9.53 GiB already allocated; 66.50 MiB free; 9.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train(train_loader, val_loader,test_loader, device =device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DCGANEncoder(in_size=(1,128,128))\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-1pHZ9zocxm"
   },
   "source": [
    "#**LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwhibs5vohGo"
   },
   "source": [
    "#**DCGAN encoder and decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8sv829K4fpj",
    "outputId": "ecedb48c-6c6a-42c6-ea1e-bbe3fa06ba7f"
   },
   "outputs": [],
   "source": [
    "x = torch.randn(100, 512, 4, 4)\n",
    "y = torch.cat([x,x],1)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m7skSZs_NO4K",
    "outputId": "7881ac19-dc78-44ea-eec9-185f47693137"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 64, 64, 64]          --\n",
      "|    └─Conv2d: 2-1                       [-1, 64, 128, 128]        576\n",
      "|    └─BatchNorm2d: 2-2                  [-1, 64, 128, 128]        128\n",
      "|    └─ReLU: 2-3                         [-1, 64, 128, 128]        --\n",
      "|    └─MaxPool2d: 2-4                    [-1, 64, 64, 64]          --\n",
      "├─Sequential: 1-2                        [-1, 64, 64, 64]          --\n",
      "|    └─ResidualBlockEncoder: 2-5         [-1, 64, 64, 64]          --\n",
      "|    |    └─Conv2d: 3-1                  [-1, 64, 64, 64]          36,864\n",
      "|    |    └─BatchNorm2d: 3-2             [-1, 64, 64, 64]          128\n",
      "|    |    └─ReLU: 3-3                    [-1, 64, 64, 64]          --\n",
      "|    |    └─Conv2d: 3-4                  [-1, 64, 64, 64]          36,864\n",
      "|    |    └─BatchNorm2d: 3-5             [-1, 64, 64, 64]          128\n",
      "|    └─ResidualBlockEncoder: 2-6         [-1, 64, 64, 64]          --\n",
      "|    |    └─Conv2d: 3-6                  [-1, 64, 64, 64]          36,864\n",
      "|    |    └─BatchNorm2d: 3-7             [-1, 64, 64, 64]          128\n",
      "|    |    └─ReLU: 3-8                    [-1, 64, 64, 64]          --\n",
      "|    |    └─Conv2d: 3-9                  [-1, 64, 64, 64]          36,864\n",
      "|    |    └─BatchNorm2d: 3-10            [-1, 64, 64, 64]          128\n",
      "├─Sequential: 1-3                        [-1, 128, 32, 32]         --\n",
      "|    └─ResidualBlockEncoder: 2-7         [-1, 128, 32, 32]         --\n",
      "|    |    └─Conv2d: 3-11                 [-1, 128, 32, 32]         73,728\n",
      "|    |    └─BatchNorm2d: 3-12            [-1, 128, 32, 32]         256\n",
      "|    |    └─ReLU: 3-13                   [-1, 128, 32, 32]         --\n",
      "|    |    └─Conv2d: 3-14                 [-1, 128, 32, 32]         147,456\n",
      "|    |    └─BatchNorm2d: 3-15            [-1, 128, 32, 32]         256\n",
      "|    |    └─Sequential: 3-16             [-1, 128, 32, 32]         8,576\n",
      "|    └─ResidualBlockEncoder: 2-8         [-1, 128, 32, 32]         --\n",
      "|    |    └─Conv2d: 3-17                 [-1, 128, 32, 32]         147,456\n",
      "|    |    └─BatchNorm2d: 3-18            [-1, 128, 32, 32]         256\n",
      "|    |    └─ReLU: 3-19                   [-1, 128, 32, 32]         --\n",
      "|    |    └─Conv2d: 3-20                 [-1, 128, 32, 32]         147,456\n",
      "|    |    └─BatchNorm2d: 3-21            [-1, 128, 32, 32]         256\n",
      "├─Sequential: 1-4                        [-1, 256, 16, 16]         --\n",
      "|    └─ResidualBlockEncoder: 2-9         [-1, 256, 16, 16]         --\n",
      "|    |    └─Conv2d: 3-22                 [-1, 256, 16, 16]         294,912\n",
      "|    |    └─BatchNorm2d: 3-23            [-1, 256, 16, 16]         512\n",
      "|    |    └─ReLU: 3-24                   [-1, 256, 16, 16]         --\n",
      "|    |    └─Conv2d: 3-25                 [-1, 256, 16, 16]         589,824\n",
      "|    |    └─BatchNorm2d: 3-26            [-1, 256, 16, 16]         512\n",
      "|    |    └─Sequential: 3-27             [-1, 256, 16, 16]         33,536\n",
      "|    └─ResidualBlockEncoder: 2-10        [-1, 256, 16, 16]         --\n",
      "|    |    └─Conv2d: 3-28                 [-1, 256, 16, 16]         589,824\n",
      "|    |    └─BatchNorm2d: 3-29            [-1, 256, 16, 16]         512\n",
      "|    |    └─ReLU: 3-30                   [-1, 256, 16, 16]         --\n",
      "|    |    └─Conv2d: 3-31                 [-1, 256, 16, 16]         589,824\n",
      "|    |    └─BatchNorm2d: 3-32            [-1, 256, 16, 16]         512\n",
      "├─Sequential: 1-5                        [-1, 512, 8, 8]           --\n",
      "|    └─ResidualBlockEncoder: 2-11        [-1, 512, 8, 8]           --\n",
      "|    |    └─Conv2d: 3-33                 [-1, 512, 8, 8]           1,179,648\n",
      "|    |    └─BatchNorm2d: 3-34            [-1, 512, 8, 8]           1,024\n",
      "|    |    └─ReLU: 3-35                   [-1, 512, 8, 8]           --\n",
      "|    |    └─Conv2d: 3-36                 [-1, 512, 8, 8]           2,359,296\n",
      "|    |    └─BatchNorm2d: 3-37            [-1, 512, 8, 8]           1,024\n",
      "|    |    └─Sequential: 3-38             [-1, 512, 8, 8]           132,608\n",
      "|    └─ResidualBlockEncoder: 2-12        [-1, 512, 8, 8]           --\n",
      "|    |    └─Conv2d: 3-39                 [-1, 512, 8, 8]           2,359,296\n",
      "|    |    └─BatchNorm2d: 3-40            [-1, 512, 8, 8]           1,024\n",
      "|    |    └─ReLU: 3-41                   [-1, 512, 8, 8]           --\n",
      "|    |    └─Conv2d: 3-42                 [-1, 512, 8, 8]           2,359,296\n",
      "|    |    └─BatchNorm2d: 3-43            [-1, 512, 8, 8]           1,024\n",
      "├─Sequential: 1-6                        [-1, 512, 4, 4]           --\n",
      "|    └─ResidualBlockEncoder: 2-13        [-1, 512, 4, 4]           --\n",
      "|    |    └─Conv2d: 3-44                 [-1, 512, 4, 4]           2,359,296\n",
      "|    |    └─BatchNorm2d: 3-45            [-1, 512, 4, 4]           1,024\n",
      "|    |    └─ReLU: 3-46                   [-1, 512, 4, 4]           --\n",
      "|    |    └─Conv2d: 3-47                 [-1, 512, 4, 4]           2,359,296\n",
      "|    |    └─BatchNorm2d: 3-48            [-1, 512, 4, 4]           1,024\n",
      "|    |    └─Sequential: 3-49             [-1, 512, 4, 4]           263,680\n",
      "|    └─ResidualBlockEncoder: 2-14        [-1, 512, 4, 4]           --\n",
      "|    |    └─Conv2d: 3-50                 [-1, 512, 4, 4]           2,359,296\n",
      "|    |    └─BatchNorm2d: 3-51            [-1, 512, 4, 4]           1,024\n",
      "|    |    └─ReLU: 3-52                   [-1, 512, 4, 4]           --\n",
      "|    |    └─Conv2d: 3-53                 [-1, 512, 4, 4]           2,359,296\n",
      "|    |    └─BatchNorm2d: 3-54            [-1, 512, 4, 4]           1,024\n",
      "├─Sequential: 1-7                        [-1, 128, 1, 1]           --\n",
      "|    └─Conv2d: 2-15                      [-1, 128, 1, 1]           1,048,704\n",
      "|    └─BatchNorm2d: 2-16                 [-1, 128, 1, 1]           256\n",
      "|    └─Tanh: 2-17                        [-1, 128, 1, 1]           --\n",
      "==========================================================================================\n",
      "Total params: 21,922,496\n",
      "Trainable params: 21,922,496\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 2.42\n",
      "==========================================================================================\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 50.13\n",
      "Params size (MB): 83.63\n",
      "Estimated Total Size (MB): 133.82\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1-1                        [-1, 64, 64, 64]          --\n",
       "|    └─Conv2d: 2-1                       [-1, 64, 128, 128]        576\n",
       "|    └─BatchNorm2d: 2-2                  [-1, 64, 128, 128]        128\n",
       "|    └─ReLU: 2-3                         [-1, 64, 128, 128]        --\n",
       "|    └─MaxPool2d: 2-4                    [-1, 64, 64, 64]          --\n",
       "├─Sequential: 1-2                        [-1, 64, 64, 64]          --\n",
       "|    └─ResidualBlockEncoder: 2-5         [-1, 64, 64, 64]          --\n",
       "|    |    └─Conv2d: 3-1                  [-1, 64, 64, 64]          36,864\n",
       "|    |    └─BatchNorm2d: 3-2             [-1, 64, 64, 64]          128\n",
       "|    |    └─ReLU: 3-3                    [-1, 64, 64, 64]          --\n",
       "|    |    └─Conv2d: 3-4                  [-1, 64, 64, 64]          36,864\n",
       "|    |    └─BatchNorm2d: 3-5             [-1, 64, 64, 64]          128\n",
       "|    └─ResidualBlockEncoder: 2-6         [-1, 64, 64, 64]          --\n",
       "|    |    └─Conv2d: 3-6                  [-1, 64, 64, 64]          36,864\n",
       "|    |    └─BatchNorm2d: 3-7             [-1, 64, 64, 64]          128\n",
       "|    |    └─ReLU: 3-8                    [-1, 64, 64, 64]          --\n",
       "|    |    └─Conv2d: 3-9                  [-1, 64, 64, 64]          36,864\n",
       "|    |    └─BatchNorm2d: 3-10            [-1, 64, 64, 64]          128\n",
       "├─Sequential: 1-3                        [-1, 128, 32, 32]         --\n",
       "|    └─ResidualBlockEncoder: 2-7         [-1, 128, 32, 32]         --\n",
       "|    |    └─Conv2d: 3-11                 [-1, 128, 32, 32]         73,728\n",
       "|    |    └─BatchNorm2d: 3-12            [-1, 128, 32, 32]         256\n",
       "|    |    └─ReLU: 3-13                   [-1, 128, 32, 32]         --\n",
       "|    |    └─Conv2d: 3-14                 [-1, 128, 32, 32]         147,456\n",
       "|    |    └─BatchNorm2d: 3-15            [-1, 128, 32, 32]         256\n",
       "|    |    └─Sequential: 3-16             [-1, 128, 32, 32]         8,576\n",
       "|    └─ResidualBlockEncoder: 2-8         [-1, 128, 32, 32]         --\n",
       "|    |    └─Conv2d: 3-17                 [-1, 128, 32, 32]         147,456\n",
       "|    |    └─BatchNorm2d: 3-18            [-1, 128, 32, 32]         256\n",
       "|    |    └─ReLU: 3-19                   [-1, 128, 32, 32]         --\n",
       "|    |    └─Conv2d: 3-20                 [-1, 128, 32, 32]         147,456\n",
       "|    |    └─BatchNorm2d: 3-21            [-1, 128, 32, 32]         256\n",
       "├─Sequential: 1-4                        [-1, 256, 16, 16]         --\n",
       "|    └─ResidualBlockEncoder: 2-9         [-1, 256, 16, 16]         --\n",
       "|    |    └─Conv2d: 3-22                 [-1, 256, 16, 16]         294,912\n",
       "|    |    └─BatchNorm2d: 3-23            [-1, 256, 16, 16]         512\n",
       "|    |    └─ReLU: 3-24                   [-1, 256, 16, 16]         --\n",
       "|    |    └─Conv2d: 3-25                 [-1, 256, 16, 16]         589,824\n",
       "|    |    └─BatchNorm2d: 3-26            [-1, 256, 16, 16]         512\n",
       "|    |    └─Sequential: 3-27             [-1, 256, 16, 16]         33,536\n",
       "|    └─ResidualBlockEncoder: 2-10        [-1, 256, 16, 16]         --\n",
       "|    |    └─Conv2d: 3-28                 [-1, 256, 16, 16]         589,824\n",
       "|    |    └─BatchNorm2d: 3-29            [-1, 256, 16, 16]         512\n",
       "|    |    └─ReLU: 3-30                   [-1, 256, 16, 16]         --\n",
       "|    |    └─Conv2d: 3-31                 [-1, 256, 16, 16]         589,824\n",
       "|    |    └─BatchNorm2d: 3-32            [-1, 256, 16, 16]         512\n",
       "├─Sequential: 1-5                        [-1, 512, 8, 8]           --\n",
       "|    └─ResidualBlockEncoder: 2-11        [-1, 512, 8, 8]           --\n",
       "|    |    └─Conv2d: 3-33                 [-1, 512, 8, 8]           1,179,648\n",
       "|    |    └─BatchNorm2d: 3-34            [-1, 512, 8, 8]           1,024\n",
       "|    |    └─ReLU: 3-35                   [-1, 512, 8, 8]           --\n",
       "|    |    └─Conv2d: 3-36                 [-1, 512, 8, 8]           2,359,296\n",
       "|    |    └─BatchNorm2d: 3-37            [-1, 512, 8, 8]           1,024\n",
       "|    |    └─Sequential: 3-38             [-1, 512, 8, 8]           132,608\n",
       "|    └─ResidualBlockEncoder: 2-12        [-1, 512, 8, 8]           --\n",
       "|    |    └─Conv2d: 3-39                 [-1, 512, 8, 8]           2,359,296\n",
       "|    |    └─BatchNorm2d: 3-40            [-1, 512, 8, 8]           1,024\n",
       "|    |    └─ReLU: 3-41                   [-1, 512, 8, 8]           --\n",
       "|    |    └─Conv2d: 3-42                 [-1, 512, 8, 8]           2,359,296\n",
       "|    |    └─BatchNorm2d: 3-43            [-1, 512, 8, 8]           1,024\n",
       "├─Sequential: 1-6                        [-1, 512, 4, 4]           --\n",
       "|    └─ResidualBlockEncoder: 2-13        [-1, 512, 4, 4]           --\n",
       "|    |    └─Conv2d: 3-44                 [-1, 512, 4, 4]           2,359,296\n",
       "|    |    └─BatchNorm2d: 3-45            [-1, 512, 4, 4]           1,024\n",
       "|    |    └─ReLU: 3-46                   [-1, 512, 4, 4]           --\n",
       "|    |    └─Conv2d: 3-47                 [-1, 512, 4, 4]           2,359,296\n",
       "|    |    └─BatchNorm2d: 3-48            [-1, 512, 4, 4]           1,024\n",
       "|    |    └─Sequential: 3-49             [-1, 512, 4, 4]           263,680\n",
       "|    └─ResidualBlockEncoder: 2-14        [-1, 512, 4, 4]           --\n",
       "|    |    └─Conv2d: 3-50                 [-1, 512, 4, 4]           2,359,296\n",
       "|    |    └─BatchNorm2d: 3-51            [-1, 512, 4, 4]           1,024\n",
       "|    |    └─ReLU: 3-52                   [-1, 512, 4, 4]           --\n",
       "|    |    └─Conv2d: 3-53                 [-1, 512, 4, 4]           2,359,296\n",
       "|    |    └─BatchNorm2d: 3-54            [-1, 512, 4, 4]           1,024\n",
       "├─Sequential: 1-7                        [-1, 128, 1, 1]           --\n",
       "|    └─Conv2d: 2-15                      [-1, 128, 1, 1]           1,048,704\n",
       "|    └─BatchNorm2d: 2-16                 [-1, 128, 1, 1]           256\n",
       "|    └─Tanh: 2-17                        [-1, 128, 1, 1]           --\n",
       "==========================================================================================\n",
       "Total params: 21,922,496\n",
       "Trainable params: 21,922,496\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 2.42\n",
       "==========================================================================================\n",
       "Input size (MB): 0.06\n",
       "Forward/backward pass size (MB): 50.13\n",
       "Params size (MB): 83.63\n",
       "Estimated Total Size (MB): 133.82\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.resnet import *\n",
    "model = Resnet18Encoder(in_size = (1, 128, 128))\n",
    "model = model.to(device)\n",
    "model\n",
    "from torchsummary import summary\n",
    "summary(model, (1,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OW2G7k2uSpCv"
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = SequentialClassifier(input_dim=28, emb_dim=64, hidden_dim=128, num_layers=2, mode=\"zeros\")\n",
    "model = latent_lstm(input_dim=64, out_dim=128, hidden_dim=128, num_layers=2, mode=\"zeros\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DvW8WI1TS0Rv",
    "outputId": "0dc0f490-9716-46c1-8d49-3d50768cdfbb"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBqqeLdNcoHt"
   },
   "source": [
    "#**Moving MNIST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9FeJXylxcr_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "# from Seq2Seq import Seq2Seq\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import io\n",
    "import imageio\n",
    "from ipywidgets import widgets, HBox\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MovingMNIST = np.load('mnist_test_seq.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MovingMNIST.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader,test_loader = MMNIST(seq_first = False,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =[]\n",
    "for i in b:\n",
    "    x.append(i)\n",
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    x[i] = x[i].unsqueeze(0)\n",
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x[0]\n",
    "for t in x[1:]:\n",
    "    y = torch.cat([y,t],0)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualizations import *\n",
    "show_gif_batch(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357,
     "referenced_widgets": [
      "7493461413af4bc6b6dc7be40466a3dd",
      "cefa09520ae64b96b609ac5d104a5770",
      "304107e3a62e4606a0c1fd76e97d0834",
      "e5e2d179eb3d4d098445bcd0f6f7f723",
      "f51140542c88454faff52395fa81f221",
      "ab951f501854459989369672bff404d7",
      "58a8bfd0ed184b8281bb3c79ad89a82c",
      "5b579c863052432b996fdacf1f4816b4",
      "a0d2588f098c4fa093ec8d852ce0f8a3",
      "b4e2241b0502428aa46d9b95639bbbb5",
      "458d62e0d9f74aca94266c7028f438ad",
      "0dbd6a91afa4403a8e2cb2953834be33",
      "8c68afcfff9a49b8ae5a486d4ac341cb",
      "b3a8817fa5514bf99db9cb8605f56dfe",
      "50aae3e4712c4bf7aaa377bed1c1c2a2",
      "42ba00e1517e43b296c988e34d454619",
      "27feb1ee56554afd863fed00e0796294",
      "4e55b95a55684eb0822d0afcb31ea1d0",
      "4dbed72f4183441f86e95fd3dd5e0f5a",
      "0dce2e75e9be4e6b982ef140f64d453d",
      "6855edbc8c26499b9d2d9d3afe83f30f",
      "79449a0f56544b7f99f45341c8d52148",
      "5411657399324d3a8bdd09b3f813e01f",
      "21cd805c6ff2484488fb768b868e5b9b",
      "5e296796e9fd4fc29df6b81344e92975",
      "e972f67898564fadaa33be85038f1946"
     ]
    },
    "id": "0la0TcGm_dwX",
    "outputId": "1476dc76-9de5-4b0e-f55e-60c183b524fb"
   },
   "outputs": [],
   "source": [
    "# Reverse process before displaying\n",
    "future_batch = future_batch.cpu().numpy() * 255.0     \n",
    "for video in future_batch.squeeze(1)[:5]:          # Loop over videos\n",
    "    with io.BytesIO() as gif:\n",
    "        imageio.mimsave(gif,video.astype(np.uint8),\"GIF\",fps=5)\n",
    "        past = Box(children=[widgets.Image(value=gif.getvalue())], layout=box_layout)\n",
    "        display(VBox([past]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sFpWdLF_6at"
   },
   "source": [
    "#**Training Fixed Prior Video Generation Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2O6EZhEB36U"
   },
   "outputs": [],
   "source": [
    "#to be added in config\n",
    "batch_size=100\n",
    "past_frames = 10\n",
    "future_frames = 10\n",
    "z_dim = 10\n",
    "g_dim = 128\n",
    "lr = 0.002\n",
    "beta = 0.0001\n",
    "hidden_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5y9pFL-ACS0"
   },
   "outputs": [],
   "source": [
    "encoder = DCGANEncoder()\n",
    "decoder = DCGANDecoder()\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "predictor = predictor_lstm(g_dim+z_dim, g_dim, hidden_dim, num_layers=2)\n",
    "posterior = latent_lstm(g_dim, z_dim, hidden_dim, num_layers=1)\n",
    "predictor = predictor.to(device)\n",
    "posterior = posterior.to(device)\n",
    "# Decay LR by a factor of 0.1 every 5 epochs\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.2)\n",
    "predictor_optimizer = torch.optim.Adam(predictor.parameters(), lr=lr, betas = (0.9, 0.999))\n",
    "posterior_optimizer = torch.optim.Adam(posterior.parameters(), lr=lr, betas = (0.9, 0.999))\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr, betas = (0.9, 0.999))\n",
    "decoder_optimizer = torch.optim.Adam(posterior.parameters(), lr=lr, betas = (0.9, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6g54h_dLH7QD"
   },
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "mse_loss = nn.MSELoss()\n",
    "# import torch.nn.functional as F\n",
    "def kld_loss(mu, log_var):\n",
    "    \"\"\"\n",
    "    Combined loss function for joint optimization of \n",
    "    prediction and ELBO\n",
    "    \"\"\"\n",
    "    # mse = F.mse_loss(predicted, real, reduction=\"sum\")\n",
    "#     recons_loss = F.binary_cross_entropy(recons.view(b_size,-1), target.view(b_size,-1), reduction='sum')\n",
    "    elbo = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    elbo /= batch_size\n",
    "    # total_loss = mse + elbo*beta\n",
    "    return elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VplKUVgNR9md"
   },
   "outputs": [],
   "source": [
    "def train_epoch(x):\n",
    "    predictor.zero_grad()\n",
    "    posterior.zero_grad()\n",
    "    encoder.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "\n",
    "    # print(\"hello\")\n",
    "\n",
    "    h_seq = [encoder(x[i]) for i in range(past_frames+future_frames)]\n",
    "    # print(h_seq[0][0].shape)\n",
    "    mse =0\n",
    "    kld = 0\n",
    "    for i in range(1,past_frames+future_frames):\n",
    "        h_target = h_seq[i][0]\n",
    "        if i<past_frames:\n",
    "            h, skip = h_seq[i-1]\n",
    "        else:\n",
    "            h = h_seq[i-1][0]\n",
    "        # print(\"here\")\n",
    "        # print(h_target.shape)\n",
    "        z_t, mu, log_var = posterior(h_target)\n",
    "        # print(\"here\")\n",
    "        # print(z_t.shape)\n",
    "        h_pred = predictor(torch.cat([h, z_t], 1))\n",
    "        x_pred = decoder([h_pred, skip])\n",
    "        mse+=mse_loss(x_pred,x[i])\n",
    "        kld+=kld_loss(mu,log_var)\n",
    "\n",
    "    total_loss = mse + kld*beta\n",
    "    total_loss.backward()\n",
    "\n",
    "    predictor_optimizer.step()\n",
    "    posterior_optimizer.step()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return mse.data.cpu().numpy()/(past_frames+future_frames), kld.data.cpu().numpy()/(past_frames+future_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "o5_wOZNKYhLW",
    "outputId": "9f4f30f4-0d6c-403b-bf61-d70343c518b3"
   },
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "for i in range(num_epochs):\n",
    "    predictor.train()\n",
    "    posterior.train()\n",
    "    encoder.train()\n",
    "    decoder.train() \n",
    "    epoch_mse =0\n",
    "    epoch_kld=0\n",
    "    epoch_loss =0\n",
    "\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for j, images in progress_bar:\n",
    "        images = images.to(device)\n",
    "\n",
    "        bs, nc, s, h, w = images.shape\n",
    "        data = images.view(s, bs, nc, h, w)\n",
    "\n",
    "        mse, kld = train_epoch(data)\n",
    "        epoch_mse+=mse\n",
    "        epoch_kld+=kld\n",
    "        progress_bar.set_description(f\"Epoch {i+1} : mse loss {epoch_mse:.5f}, kld los {epoch_kld: .5f} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "bBqqeLdNcoHt"
   ],
   "name": "video-prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0dbd6a91afa4403a8e2cb2953834be33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b3a8817fa5514bf99db9cb8605f56dfe"
      ],
      "layout": "IPY_MODEL_8c68afcfff9a49b8ae5a486d4ac341cb"
     }
    },
    "0dce2e75e9be4e6b982ef140f64d453d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ImageModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ImageModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ImageView",
      "format": "png",
      "height": "",
      "layout": "IPY_MODEL_6855edbc8c26499b9d2d9d3afe83f30f",
      "width": ""
     }
    },
    "21cd805c6ff2484488fb768b868e5b9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "BoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "BoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "BoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5e296796e9fd4fc29df6b81344e92975"
      ],
      "layout": "IPY_MODEL_e5e2d179eb3d4d098445bcd0f6f7f723"
     }
    },
    "27feb1ee56554afd863fed00e0796294": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4dbed72f4183441f86e95fd3dd5e0f5a"
      ],
      "layout": "IPY_MODEL_4e55b95a55684eb0822d0afcb31ea1d0"
     }
    },
    "304107e3a62e4606a0c1fd76e97d0834": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "BoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "BoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "BoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f51140542c88454faff52395fa81f221"
      ],
      "layout": "IPY_MODEL_e5e2d179eb3d4d098445bcd0f6f7f723"
     }
    },
    "37574759ccb64992a21fd449b660fb13": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42ba00e1517e43b296c988e34d454619": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "458d62e0d9f74aca94266c7028f438ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4dbed72f4183441f86e95fd3dd5e0f5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "BoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "BoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "BoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0dce2e75e9be4e6b982ef140f64d453d"
      ],
      "layout": "IPY_MODEL_e5e2d179eb3d4d098445bcd0f6f7f723"
     }
    },
    "4e55b95a55684eb0822d0afcb31ea1d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50aae3e4712c4bf7aaa377bed1c1c2a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ImageModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ImageModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ImageView",
      "format": "png",
      "height": "",
      "layout": "IPY_MODEL_42ba00e1517e43b296c988e34d454619",
      "width": ""
     }
    },
    "5411657399324d3a8bdd09b3f813e01f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58a8bfd0ed184b8281bb3c79ad89a82c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a0d2588f098c4fa093ec8d852ce0f8a3"
      ],
      "layout": "IPY_MODEL_5b579c863052432b996fdacf1f4816b4"
     }
    },
    "5b579c863052432b996fdacf1f4816b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e296796e9fd4fc29df6b81344e92975": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ImageModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ImageModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ImageView",
      "format": "png",
      "height": "",
      "layout": "IPY_MODEL_e972f67898564fadaa33be85038f1946",
      "width": ""
     }
    },
    "6855edbc8c26499b9d2d9d3afe83f30f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7493461413af4bc6b6dc7be40466a3dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_304107e3a62e4606a0c1fd76e97d0834"
      ],
      "layout": "IPY_MODEL_cefa09520ae64b96b609ac5d104a5770"
     }
    },
    "79449a0f56544b7f99f45341c8d52148": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_21cd805c6ff2484488fb768b868e5b9b"
      ],
      "layout": "IPY_MODEL_5411657399324d3a8bdd09b3f813e01f"
     }
    },
    "7a82bc2cf62440939d84fa17789a06d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ImageModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ImageModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ImageView",
      "format": "png",
      "height": "",
      "layout": "IPY_MODEL_b96d5d8896b44ab5918e3f0748fa768a",
      "width": ""
     }
    },
    "8a3be34d5abc465086a6f993232c85c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ImageModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ImageModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ImageView",
      "format": "png",
      "height": "",
      "layout": "IPY_MODEL_937bc8ae810f4e4981287e34a9ff3ec0",
      "width": ""
     }
    },
    "8c68afcfff9a49b8ae5a486d4ac341cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "937bc8ae810f4e4981287e34a9ff3ec0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0d2588f098c4fa093ec8d852ce0f8a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "BoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "BoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "BoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b4e2241b0502428aa46d9b95639bbbb5"
      ],
      "layout": "IPY_MODEL_e5e2d179eb3d4d098445bcd0f6f7f723"
     }
    },
    "ab951f501854459989369672bff404d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3a8817fa5514bf99db9cb8605f56dfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "BoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "BoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "BoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_50aae3e4712c4bf7aaa377bed1c1c2a2"
      ],
      "layout": "IPY_MODEL_e5e2d179eb3d4d098445bcd0f6f7f723"
     }
    },
    "b4e2241b0502428aa46d9b95639bbbb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ImageModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ImageModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ImageView",
      "format": "png",
      "height": "",
      "layout": "IPY_MODEL_458d62e0d9f74aca94266c7028f438ad",
      "width": ""
     }
    },
    "b96d5d8896b44ab5918e3f0748fa768a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cefa09520ae64b96b609ac5d104a5770": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5e2d179eb3d4d098445bcd0f6f7f723": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": "row",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e727ed3736704c3b867e7c8fac64e3a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8a3be34d5abc465086a6f993232c85c5",
       "IPY_MODEL_7a82bc2cf62440939d84fa17789a06d7"
      ],
      "layout": "IPY_MODEL_37574759ccb64992a21fd449b660fb13"
     }
    },
    "e972f67898564fadaa33be85038f1946": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f51140542c88454faff52395fa81f221": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ImageModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ImageModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ImageView",
      "format": "png",
      "height": "",
      "layout": "IPY_MODEL_ab951f501854459989369672bff404d7",
      "width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
